# -*- coding: utf-8 -*-
"""Fifa19_gradient_descent.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-66VTvbH_f7TU_bTkEyRg0Rw3xi30YML
"""

import numpy as np
import scipy as sp
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.offline as py
import plotly.graph_objs as go
from plotly import tools
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
import cufflinks as cf
import warnings
warnings.filterwarnings('ignore')

init_notebook_mode(connected=True)
cf.go_offline()



from google.colab import drive
drive.mount('/content/drive')

data= pd.read_csv('/content/sample_data/Fifa_19dataset.csv',encoding='iso-8859-1')

data

data.info()

print('Number of Categorical Columns: ', len(data.select_dtypes(include=object).columns))
print('Number of Numerical Columns: ', len(data.select_dtypes(exclude=object).columns))

"""Data wrangling

"""

data.drop(columns=['Unnamed: 0', 'ID',  'Special', 'Real Face', 'Release Clause',
                   'Joined', 'Contract Valid Until'], inplace=True)

#Check for missing values in columns where missing values is more than half of the total number of values.
data.isnull().sum()[data.isnull().sum() >= 9000]

data.drop(columns=['Loaned From'], inplace=True)

data.isnull().sum()

#Players with no club.
data['Club'].fillna(value='No Club', inplace=True)

data[data['Preferred Foot'].isna()].head()

#Full of NaN values for many features, so drop.
data.drop(index=data[data['Preferred Foot'].isna()].index, inplace=True)

data[data['Position'].isna()][['Name', 'Nationality', 'LS', 'ST','RS', 'LW', 'LF', 'CF', 'RF', 'RW',
                              'LAM', 'CAM', 'RAM', 'LM', 'LCM','CM', 'RCM', 'RM', 'LWB', 'LDM', 'CDM',
                              'RDM', 'RWB', 'LB', 'LCB', 'CB', 'RCB', 'RB']].head()

#Can fill in position manually but LS, RS, CF, etc. features have no values, so drop them.
data.drop(index=data[data['Position'].isna()].index, inplace=True)

#Checking the number of missing values in the remaining columns.
data.isnull().sum()[data.isnull().sum() > 0]

len(data[data['Position'] == 'GK'])

data.fillna(value=0, inplace=True)

data.isnull().sum().sum()

data.select_dtypes(include=object).columns

def currencyConverter(val):
    if val[-1] == 'M':
        val = val[1:-1]
        val = float(val) * 1000000
        return val

    elif val[-1] == 'K':
        val = val[1:-1]
        val = float(val) * 1000
        return val

    else:
        return 0

data['Value in Pounds'] = data['Value'].apply(currencyConverter)
data['Wage in Pounds'] = data['Wage'].apply(currencyConverter)

data.drop(columns=['Value', 'Wage'], inplace=True)

data.head()

data[['LS', 'ST', 'RS', 'LW', 'LF', 'CF', 'RF', 'RW', 'LAM', 'CAM',
       'RAM', 'LM', 'LCM', 'CM', 'RCM', 'RM', 'LWB', 'LDM', 'CDM', 'RDM',
       'RWB', 'LB', 'LCB', 'CB', 'RCB', 'RB']].head()

#Function to convert skill rating at each position.
def skillConverter(val):
    if type(val) == str:
        s1 = val[0:2]
        s2 = val[-1]
        val = int(s1) + int(s2)
        return val

    else:
        return val

skill_columns = ['LS', 'ST', 'RS', 'LW', 'LF', 'CF', 'RF', 'RW', 'LAM', 'CAM',
       'RAM', 'LM', 'LCM', 'CM', 'RCM', 'RM', 'LWB', 'LDM', 'CDM', 'RDM',
       'RWB', 'LB', 'LCB', 'CB', 'RCB', 'RB']

for col in skill_columns:
    data[col] = data[col].apply(skillConverter)

data[['LS', 'ST', 'RS', 'LW', 'LF', 'CF', 'RF', 'RW', 'LAM', 'CAM',
       'RAM', 'LM', 'LCM', 'CM', 'RCM', 'RM', 'LWB', 'LDM', 'CDM', 'RDM',
       'RWB', 'LB', 'LCB', 'CB', 'RCB', 'RB']].head()

data[['Height', 'Weight']].head()

def height_converter(val):
    f = val.split("'")[0]
    i = val.split("'")[1]
    h = (int(f) * 30.48) + (int(i)*2.54)
    return h

def weight_converter(val):
    w = int(val.split('lbs')[0])
    return w

data['Height in Cms'] = data['Height'].apply(height_converter)
data['Weight in Pounds'] = data['Weight'].apply(weight_converter)

data.drop(columns=['Height', 'Weight'], inplace=True)
data[['Height in Cms', 'Weight in Pounds']].head()

data['Work Rate'].unique()

data['Body Type'].unique()

data['Body Type'][data['Body Type'] == 'Messi'] = 'Lean'
data['Body Type'][data['Body Type'] == 'C. Ronaldo'] = 'Normal'
data['Body Type'][data['Body Type'] == 'Neymar'] = 'Lean'
data['Body Type'][data['Body Type'] == 'Courtois'] = 'Lean'
data['Body Type'][data['Body Type'] == 'PLAYER_BODY_TYPE_25'] = 'Normal'
data['Body Type'][data['Body Type'] == 'Shaqiri'] = 'Stocky'
data['Body Type'][data['Body Type'] == 'Akinfenwa'] = 'Stocky'

print(data['Position'].unique())
print(data['Position'].nunique())

def position_simplifier(val):

    if val == 'RF' or val == 'ST' or val == 'LF' or val == 'RS' or val == 'LS' or val == 'CF':
        val = 'F'
        return val

    elif val == 'LW' or val == 'RCM' or val == 'LCM' or val == 'LDM' or val == 'CAM' or val == 'CDM' or val == 'RM' \
         or val == 'LAM' or val == 'LM' or val == 'RDM' or val == 'RW' or val == 'CM' or val == 'RAM':
        val = 'M'
        return val


    elif val == 'RCB' or val == 'CB' or val == 'LCB' or val == 'LB' or val == 'RB' or val == 'RWB' or val == 'LWB':
        val = 'D'
        return val

    else:
        return val

data['Position'] = data['Position'].apply(position_simplifier)
data['Position'].value_counts()

"""*Visualization *

"""

plt.hist(data['Age'],bins=55,edgecolor='black', alpha=0.99)
plt.xlabel('Age')
plt.ylabel('Count')
plt.title('Players Age Distribution ')
# Show the plot
plt.show()

fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)

ax1.hist(data['Height in Cms'], bins=30, alpha=0.99, label='Height of player')
ax2.hist(data['Weight in Pounds'], bins=30, alpha=0.99, label='weight of player')

# Add a legend, labels, and title
ax1.legend(loc='upper right')
ax1.set_xlabel('height')
ax1.set_ylabel('height')

ax2.legend(loc='upper right')
ax2.set_xlabel('Weight')
ax2.set_ylabel('Weight')
plt.title('Height & Weight Distribution')


plt.show()

# Define the data for the pie chart

# Create the pie chart
# plt.pie(data['Position'].value_counts().values,
#                 labels=data['Position'].value_counts().index.values,
#                  colors=5, autopct='%1.1f%%', startangle=90)

# # Add a title
# plt.title('Distribution of Players Position-Wise')

# # Show the chart
# plt.show()

"""Starting classification and predict on favorable position (position)

"""

from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.linear_model import SGDRegressor , SGDClassifier
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, mean_squared_error, r2_score

def pos_numeric(val):
    if val == 'GK':
        return 0
    elif val == 'D':
        return 1
    elif val == 'M':
        return 2
    else:
        return 3

data['Position'] = data['Position'].apply(pos_numeric)

df_pos = data.copy()

#Dropping unnecessary columns
df_pos.drop(columns=['Name', 'Nationality', 'Club'], inplace=True)

X = df_pos.drop(columns=['Position'])
X = pd.get_dummies(X)
y = df_pos['Position']

#Splitting dataset into train and test sets Best tune yet.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

from sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error
from sklearn.metrics import explained_variance_score

gbclassifier = GradientBoostingClassifier()

gbclassifier.fit(X_train, y_train)

prediction = gbclassifier.predict(X_test)

print(classification_report(y_test, prediction))
print('\n')
print(confusion_matrix(y_test, prediction))
print('\n')
print('Accuracy Score: ', accuracy_score(y_test, prediction))

"""Predicting whole data using linear regression after dropping the clue data ( name / nationality / club )"""

df_ovr = data.copy()
df_ovr.drop(columns=['Name', 'Nationality', 'Club'], inplace=True)

X = df_ovr.drop(columns=['Overall'])
X = pd.get_dummies(X)
y = df_ovr['Overall']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

linmodel = LinearRegression()

linmodel.fit(X_train, y_train)

pred = linmodel.predict(X_test)

print('RMSE:', np.sqrt(mean_squared_error(y_test, pred)))
print('r^2 score: ', r2_score(y_test, pred))

"""Predicting upon whole data after cleaning it and removing clues compared with pervious linear and gradient model

"""

X = df_ovr.drop(columns=['Overall'])
X = pd.get_dummies(X)
y = df_ovr['Overall']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

gbregressor = GradientBoostingRegressor()

gbregressor.fit(X_train, y_train)

pred = gbregressor.predict(X_test)

red = gbregressor.predict(X_test)
print('RMSE:', np.sqrt(mean_squared_error(y_test, pred)))
print('r^2 score: ', r2_score(y_test, pred))

gbr1 = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

gbr1.fit(X_train, y_train)

pr1 = gbr1.predict(X_test)

r1 = gbr1.predict(X_test)
print('RMSE:', np.sqrt(mean_squared_error(y_test, pr1)))
print('r^2 score: ', r2_score(y_test, pr1))

gbr2 = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.01, max_depth=3, random_state=30)

gbr2.fit(X_train, y_train)

pr2 = gbr2.predict(X_test)

r2 = gbr2.predict(X_test)
print('RMSE:', np.sqrt(mean_squared_error(y_test, pr2)))
print('r^2 score: ', r2_score(y_test, pr2))



gbr3 = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.001, max_depth=5, random_state=42)

gbr3.fit(X_train, y_train)

pr3 = gbr3.predict(X_test)

r3 = gbr3.predict(X_test)
print('RMSE:', np.sqrt(mean_squared_error(y_test, pr3)))
print('r^2 score: ', r2_score(y_test, pr3))

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import metrics
#Loading data

X=data['Position']
Y=data['Overall']
print(X.shape)
L = 0.0000001  # The learning Rate
epochs = 100  # The number of iterations to perform gradient descent
m=0
c=0
n = float(len(X)) # Number of elements in X
for i in range(epochs):
    Y_pred = m*X + c  # The current predicted value of Y
    D_m = (-2/n) * sum((Y - Y_pred)* X)  # Derivative wrt m
    D_c = (-2/n) * sum(Y - Y_pred)  # Derivative wrt c
    m = m - L * D_m  # Update m
    c = c - L * D_c  # Update c

prediction = m*X + c
plt.scatter(X, Y)
plt.xlabel('position', fontsize = 20)
plt.ylabel('overall', fontsize = 20)
plt.plot(X, prediction, color='red', linewidth = 3)
plt.show()

print('Mean Square Error', metrics.mean_squared_error(Y, prediction))

#Predict your GPA based on your SAT Score
STA_Score=input('Enter your Position: ')
y_test=m*F(STA_Score) + c
print('Your predicted Overall is ' + y_test)

